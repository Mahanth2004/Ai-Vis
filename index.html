<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-Visualization Lab-Web</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <h1>
        <!-- Lab Experiments: -->
    </h1>
    
<div class="exp_code">
    <div class="codes">
        <h3>Exp 1- Techniques for data preprocessing</h3>
        <pre>
            <b style="color: green;">A.</b>
            .import csv
import statistics
def remove_mean(input_file,output_file):
    with open(input_file,'r') as file:
        reader=csv.reader(file)
        data=[float(row[0]) for row in reader]
    mean_value=statistics.mean(data)
    mean_removed_data=[x-mean_value for x in data]
    with open(output_file,'w',newline="")as file:
        writer=csv.writer(file)
        for value in mean_removed_data:
            writer.writerow([value])
if __name__=="__main__":
    input_file="data.csv"
    output_file="mean_removed_data"
    remove_mean(input_file,output_file)

    <b style="color: green;">B.</b>
    .import csv
    import statistics
    def remove_mean(input_file,output_file):
        with open(input_file,'r') as file:
            reader=csv.reader(file)
            data =[float(row[0]) for row in reader]
        mean_value=statistics.mean(data)
        mean_removed_data=[x-mean_value for x in data]
        with open (output_file,'w',newline='')as file:
            writer=csv.writer(file)
            for value in mean_removed_data:
                writer.writerow([value])
    if __name__ =="__main__":
        input_file=r"data.csv"
        output_file="mean_removed_data.csv"
        
        <b style="color: green;">C.</b>
    import csv
import statistics
def z_score_normalization(input_file,output_file):
    with open(input_file,'r') as file:
        reader =csv.reader(file)
        data=[float(row[0]) for row in reader]
    mean_value=statistics.mean(data)
    standard_dev=statistics.stdev(data)
    z_scored_data=[(x-mean_value)/standard_dev for x in data]                                                                          
    with open(output_file,'w',newline="") as file:
        writer = csv.writer(file)
        for value in z_scored_data:
            writer.writerow([value])
if __name__ == "__main__":
    input_file="data.csv"
    output_file = "z_normalization_data.csv"
    z_score_normalization(input_file,output_file)
        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2a -  2.a.  Na√Øve Bayes Classifier</h3>
        <pre>
           
import pandas as pd
f = pd.DataFrame({'Weather':['Sunny', 'Rainy', 'Sunny', 'Sunny'],
                'Wind':['Mild', 'Mild', 'High', 'Mild'],
                'Temp':['Moderate', 'Mild', 'Moderate', 'Mild'],
                'go':['Yes', 'No', 'Yes', 'Yes']})
print(f.columns)

from sklearn.naive_bayes import GaussianNB as g
from sklearn.preprocessing import LabelEncoder as le
from sklearn.model_selection import train_test_split as tt

l = le()
for i in f.columns:
    f[i] = l.fit_transform(f[i])
    x = f.iloc[:, :3]
    y = f.iloc[:, 3]

xtr, xte, ytr, yte = tt(x, y, test_size=0.3)
gg = g()
gg.fit(xtr, ytr)
y_pred = gg.predict(xte)

from sklearn.metrics import accuracy_score
print(accuracy_score(yte, y_pred))

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2b - 2.b.Support vector machine</h3>
        <pre>
            import numpy as np
            import matplotlib.pyplot as plt
            import pandas as pd
            dataset = pd.read_csv("Social_Network _Ads1.csv")
            X = dataset.iloc[:, [2, 3]].values 
            y = dataset.iloc[:, 4].values
            from sklearn.model_selection import train_test_split 
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
            from sklearn.preprocessing import StandardScaler
            sc = StandardScaler()
            X_train = sc.fit_transform(X_train)
            X_test = sc.transform(X_test)
            from sklearn.svm import SVC
            classifier = SVC(kernel='rbf', random_state = 0)
            classifier.fit(X_train, y_train)
            y_pred = classifier.predict(X_test)
            from sklearn.metrics import confusion_matrix, accuracy_score
            cm = confusion_matrix(y_test, y_pred)
            print(cm)
            accuracy_score(y_test,y_pred)
            from matplotlib.colors import ListedColormap
            X_set, y_set = X_test, y_test
            X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
            plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),alpha = 0.75, cmap = ListedColormap(('red', 'green')))
            plt.xlim(X1.min(), X1.max())
            plt.ylim(X2.min(), X2.max())
            for i, j in enumerate(np.unique(y_set)):
                plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],c = ListedColormap(('pink', 'green'))(i), label = j)
            plt.title('SVM (Test set)')
            plt.xlabel('Age')
            plt.ylabel('Estimated Salary')
            plt.legend()
            plt.show()
            
        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2c - 2.c.Logistic regression</h3>
        <pre>
            import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from matplotlib.colors import ListedColormap

dataset = pd.read_csv("diabetes1.csv")

x = dataset.iloc[:, [4, 7]].values
y = dataset.iloc[:, 8].values

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25, random_state = 0)

sc_x = StandardScaler()
xtrain = sc_x.fit_transform(xtrain)
xtest = sc_x.transform(xtest)

print (xtrain[0:10, :])

classifier = LogisticRegression(random_state = 0)
classifier.fit(xtrain, ytrain)
y_pred = classifier.predict(xtest)

cm = confusion_matrix(ytest, y_pred)

print ("Confusion Matrix : \n", cm)

print ("Accuracy : ", accuracy_score(ytest, y_pred))

X_set, y_set = xtest, ytest
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, 
          stop = X_set[:, 0].max() + 1, step = 0.01),
          np.arange(start = X_set[:, 1].min() - 1,
          stop = X_set[:, 1].max() + 1, step = 0.01))

plt.contourf(X1, X2, classifier.predict(
      np.array([X1.ravel(), X2.ravel()]).T).reshape(
      X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))

plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())

for i, j in enumerate(np.unique(y_set)):
  plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
        c = ListedColormap(('red', 'green'))(i), label = j)
  
plt.title('Classifier (Test set)')
plt.xlabel('Age')
plt.ylabel('Diabetes')
plt.legend()
plt.show()

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2d - 2.d.DECISION TREE</h3>
        <pre>
            import pandas as pd
import math
import numpy as np

data = pd.read_csv("3-dataset.csv")
features = [feat for feat in data]
features.remove("answer")

class Node:
    def __init__(self):
        self.children = []
        self.value = ""
        self.isLeaf = False
        self.pred = ""
def entropy(examples):
    pos = 0.0
    neg = 0.0
    for _, row in examples.iterrows():
        if row["answer"] == "yes":
            pos += 1
        else:
            neg += 1
    if pos == 0.0 or neg == 0.0:
        return 0.0
    else:
        p = pos / (pos + neg)
        n = neg / (pos + neg)
        return -(p * math.log(p, 2) + n * math.log(n, 2))

    
def info_gain(examples, attr):
    uniq = np.unique(examples[attr])
    gain = entropy(examples)
    for u in uniq:
        subdata = examples[examples[attr] == u]
        sub_e = entropy(subdata)
        gain -= (float(len(subdata)) / float(len(examples))) * sub_e
    return gain

def ID3(examples, attrs):
    root = Node()

    max_gain = 0
    max_feat = ""
    for feature in attrs:
        gain = info_gain(examples, feature)
        if gain > max_gain:
            max_gain = gain
            max_feat = feature
    root.value = max_feat
    uniq = np.unique(examples[max_feat])
    for u in uniq:
        subdata = examples[examples[max_feat] == u]
        if entropy(subdata) == 0.0:
            newNode = Node()
            newNode.isLeaf = True
            newNode.value = u
            newNode.pred = np.unique(subdata["answer"])
            root.children.append(newNode)
        else:
            dummyNode = Node()
            dummyNode.value = u
            new_attrs = attrs.copy()
            new_attrs.remove(max_feat)
            child = ID3(subdata, new_attrs)
            dummyNode.children.append(child)
            root.children.append(dummyNode)

    return root

def printTree(root: Node, depth=0):
    for i in range(depth):
        print("\t", end="")
    print(root.value, end="")
    if root.isLeaf:
        print(" -> ", root.pred)
    print()
    for child in root.children:
        printTree(child, depth + 1)

def classify(root: Node, new):
    for child in root.children:
        if child.value == new[root.value]:
            if child.isLeaf:
                print ("Predicted Label for new example", new," is:", child.pred)
                exit
            else:
                classify (child.children[0], new)

root = ID3(data, features)
print("Decision Tree is:")
printTree(root)
print ("------------------")

new = {"outlook":"sunny", "temperature":"hot", "humidity":"normal", "wind":"strong"}
classify (root, new)                

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2e - 2.e.Random forest</h3>
        <pre>
            import pandas as pd
data=pd.read_csv("HeartDisease1.csv")
X =data.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12]].values
y =data.iloc[:,13].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier()
rfc.fit(X_train, y_train)
y_pred=rfc.predict(X_test)
from sklearn import metrics
print("Classification Accuracy:", metrics.accuracy_score(y_test, y_pred)*100)
cm=metrics.confusion_matrix(y_test,y_pred)
print(cm)
import seaborn as sn
from matplotlib import pyplot as plt
plt.figure(figsize=(5,4))
sn.heatmap(cm,annot=True)
plt.xlabel('Predicted value')
plt.ylabel('Actual value')
plt.show()

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 3 - K-MEANS</h3>
        <pre>
            import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.cluster import KMeans 
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler
iris = pd.read_csv("Iris1.csv")
x = iris.iloc[:, [ 1,2,3,4]].values

from sklearn.cluster import KMeans
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)


kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(x)
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'blue', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'orange', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')

plt.legend()

plt.show()

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 5</h3>
        <pre>
            import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist

nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_words = [word.lower() for word in word_tokens if word.isalpha() and word.lower() not in stop_words]
    return filtered_words

def create_bow_model(texts):
    all_words = []
    for text in texts:
        words = preprocess_text(text)
        all_words.extend(words)

    word_freq = FreqDist(all_words)
    bow_model = {word: freq for word, freq in word_freq.items()}
    return bow_model

# Example usage
texts = [
    "The cat sat on the mat, and the mat was comfortable.",
    "She sang a sweet song, a song that touched everyone's heart.",
    "Coding coding can be challenging, but coding is also incredibly rewarding.",
]

bow_model = create_bow_model(texts)

# Print the Bag of Words model
print("Bag of Words Model:")
for word, freq in bow_model.items():
    print(f"{word}: {freq}")

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 6</h3>
       <pre>
        import csv
        import re
        def identify_patterns(csv_file_path, column_name):
            patterns = {}
            with open(csv_file_path, 'r') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    text = row[column_name]
        
                    pattern_matches = re.findall(r'Female', text, flags=re.IGNORECASE)
        
                    for match in pattern_matches:
                        if match in patterns:
                            patterns[match] += 1
                        else:
                            patterns[match] = 1
            return patterns
        csv_file_path = 'Social_Network _Ads.csv'
        column_name = 'Gender'    
        result = identify_patterns(csv_file_path, column_name)
        for pattern, count in result.items():
            print(f"Pattern: {pattern}, Count: {count}")
       </pre>



    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 7</h3>
       <pre>
       import gensim.downloader as api
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

word_vectors = api.load("glove-wiki-gigaword-100")
sentences = [
    "Natural language processing is a challenging but fascinating field.",
    "Word embeddings capture semantic meanings of words in a vector space."
]

tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]

for tokenized_sentence in tokenized_sentences:
    for word in tokenized_sentence:
        if word in word_vectors:
            similar_words = word_vectors.most_similar(word)
            print(f"Words similar to '{word}': {similar_words}")
        else:
            print(f"'{word}' is not in the pre-trained Word2Vec model.")
       </pre>

    </div><br/><hr/>
    

    <div class="codes">
        <h3>Exp 8</h3>
        <pre>
            def aStarAlgo(start_node, stop_node):
         
        open_set = set(start_node) 
        closed_set = set()
        g = {} #store distance from starting node
        parents = {}# parents contains an adjacency map of all nodes
 
        #ditance of starting node from itself is zero
        g[start_node] = 0
        #start_node is root node i.e it has no parent nodes
        #so start_node is set to its own parent node
        parents[start_node] = start_node
         
         
        while len(open_set) > 0:
            n = None
 
            #node with lowest f() is found
            for v in open_set:
                if n == None or g[v] + heuristic(v) < g[n] + heuristic(n):
                    n = v
             
                     
            if n == stop_node or Graph_nodes[n] == None:
                pass
            else:
                for (m, weight) in get_neighbors(n):
                    #nodes 'm' not in first and last set are added to first
                    #n is set its parent
                    if m not in open_set and m not in closed_set:
                        open_set.add(m)
                        parents[m] = n
                        g[m] = g[n] + weight
                         
     
                    #for each node m,compare its distance from start i.e g(m) to the
                    #from start through n node
                    else:
                        if g[m] > g[n] + weight:
                            #update g(m)
                            g[m] = g[n] + weight
                            #change parent of m to n
                            parents[m] = n
                             
                            #if m in closed set,remove and add to open
                            if m in closed_set:
                                closed_set.remove(m)
                                open_set.add(m)
 
            if n == None:
                print('Path does not exist!')
                return None
 
            # if the current node is the stop_node
            # then we begin reconstructin the path from it to the start_node
            if n == stop_node:
                path = []
 
                while parents[n] != n:
                    path.append(n)
                    n = parents[n]
 
                path.append(start_node)
 
                path.reverse()
 
                print('Path found: {}'.format(path))
                return path
 
 
            # remove n from the open_list, and add it to closed_list
            # because all of his neighbors were inspected
            open_set.remove(n)
            closed_set.add(n)
 
        print('Path does not exist!')
        return None
         
#define fuction to return neighbor and its distance
#from the passed node
def get_neighbors(v):
    if v in Graph_nodes:
        return Graph_nodes[v]
    else:
        return None
#for simplicity we ll consider heuristic distances given
#and this function returns heuristic distance for all nodes
def heuristic(n):
        H_dist = {
            'A': 11,
            'B': 6,
            'C': 99,
            'D': 1,
            'E': 7,
            'G': 0,
             
        }
 
        return H_dist[n]
 
#Describe your graph here  
Graph_nodes = {
    'A': [('B', 2), ('E', 3)],
    'B': [('C', 1),('G', 9)],
    'C': None,
    'E': [('D', 6)],
    'D': [('G', 1)],
     
}
aStarAlgo('A', 'G')

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 9</h3>
        <pre>
            def print_board(board):
    for row in board:
        print(" | ".join(row))
        print("-" * 5)

def check_winner(board, player):
    # Check rows, columns, and diagonals
    for i in range(3):
        if all(board[i][j] == player for j in range(3)) or all(board[j][i] == player for j in range(3)):
            return True
    if all(board[i][i] == player for i in range(3)) or all(board[i][2 - i] == player for i in range(3)):
        return True
    return False

def is_board_full(board):
    return all(board[i][j] != " " for i in range(3) for j in range(3))

def tic_tac_toe():
    board = [[" " for _ in range(3)] for _ in range(3)]
    players = ["X", "O"]
    current_player = players[0]

    while True:
        print_board(board)

        # Get player move
        while True:
            row = int(input("Enter row (0, 1, or 2): "))
            col = int(input("Enter column (0, 1, or 2): "))
            if 0 <= row < 3 and 0 <= col < 3 and board[row][col] == " ":
                break
            else:
                print("Invalid move. Try again.")

        # Make the move
        board[row][col] = current_player

        # Check for a winner
        if check_winner(board, current_player):
            print_board(board)
            print(f"Player {current_player} wins!")
            break

        # Check for a tie
        if is_board_full(board):
            print_board(board)
            print("It's a tie!")
            break

        # Switch to the other player
        current_player = players[1] if current_player == players[0] else players[0]

if __name__ == "__main__":
    tic_tac_toe()


        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 10</h3>
        <pre>
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_test = StandardScaler().fit_transform(X_train), StandardScaler().fit_transform(X_test)
model_single_layer = models.Sequential([layers.Dense(64, 'relu', input_shape=(4,)), layers.Dense(3, 'softmax')])
model_single_layer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model_single_layer.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test))
single_layer_accuracy = accuracy_score(y_test, np.argmax(model_single_layer.predict(X_test), axis=1))
print(f"\nSingle-layer Neural Network - Accuracy: {single_layer_accuracy}")
model_multi_layer = models.Sequential([layers.Dense(64, 'relu', input_shape=(4,)), layers.Dense(32, 'relu'), layers.Dense(3, 'softmax')])
model_multi_layer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model_multi_layer.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test))
multi_layer_accuracy = accuracy_score(y_test, np.argmax(model_multi_layer.predict(X_test), axis=1))
print(f"\nMulti-layer Neural Network - Accuracy: {multi_layer_accuracy}")
        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 11</h3>
        <pre>

        
        import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)
X_train = np.random.rand(100, 1)
y_train = 2 * X_train + 1 + 0.1 * np.random.randn(100, 1)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=1, input_shape=(1,))
])
model.compile(optimizer='sgd', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, verbose=0)
plt.plot(history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error Loss')
plt.title('Training Loss')
plt.show()
X_test = np.array([[0.2], [0.5], [0.8]])
predictions = model.predict(X_test)
for i in range(len(X_test)):
    print(f"Input: {X_test[i][0]}, Predicted Output: {predictions[i][0]}")
</pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 12</h3>
        <pre>
            import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D
# Load and preprocess the MNIST dataset
mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize pixel values
# Define the CNN model
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
# Train the model
model.fit(X_train, y_train, epochs=5, validation_split=0.1)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'\nTest accuracy: {test_acc}')


        </pre>
    </div><br/><hr/>
                <h1>NLP-Source Codes</h1>
<div class="codes">
    <h3>Exp 1</h3>
    <pre>
        
import spacy

nlp=spacy.load("en_core_web_sm")
text="Natural language Processing is a Fascinating field of study."

doc=nlp(text)
tokens=[token.text for token in doc]
lemmas=[token.lemma_ for token in doc]

print("Tokens:",tokens)
print("Lemmas:",lemmas)


for token in doc:
    print(token.text,token.dep_,token.head.text,token.head.pos_,[child for child in token.children])
    
Tokens: ['Natural', 'language', 'Processing', 'is', 'a', 'Fascinating', 'field', 'of', 'study', '.']
Lemmas: ['natural', 'language', 'processing', 'be', 'a', 'fascinating', 'field', 'of', 'study', '.']
Natural amod language NOUN []
language compound Processing NOUN [Natural]
Processing nsubj is AUX [language]
is ROOT is AUX [Processing, field, .]
a det field NOUN []
Fascinating amod field NOUN []
field attr is AUX [a, Fascinating, of]
of prep field NOUN [study]
study pobj of ADP []
. punct is AUX []
 
        
    </pre>
</div><br/><hr/>
<h1>NLP-Source Codes</h1>
<div class="codes">
    <h3>Exp 2</h3>
    <pre>
        
import spacy
nlp = spacy.load("en_core_web_sm")
customer_feedback = [
 "The product is amazing! I love the quality.",
 "The customer service was terrible, very disappointed.",
 "Great experience overall, highly recommended.",
 "The delivery was late, very frustrating."
]
def analyze_feedback(feedback):
     for idx, text in enumerate(feedback, start=1):
            print(f"\nAnalyzing Feedback {idx}: '{text}'")
            doc = nlp(text)
            # Extract tokens and lemmatization
            tokens = [token.text for token in doc]
            lemmas = [token.lemma_ for token in doc]
            print("Tokens:", tokens)
            print("Lemmas:", lemmas)
            # Dependency parsing
            print("\nDependency Parsing:")
            for token in doc:
                print(token.text, token.dep_, token.head.text, token.head.pos_,
                [child for child in token.children])
if __name__ == "__main__":
    
     analyze_feedback(customer_feedback)
Analyzing Feedback 1: 'The product is amazing! I love the quality.'
Tokens: ['The', 'product', 'is', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']
Lemmas: ['the', 'product', 'be', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']

Dependency Parsing:
The det product NOUN []
product nsubj is AUX [The]
is ROOT is AUX [product, amazing, !]
amazing acomp is AUX []
! punct is AUX []
I nsubj love VERB []
love ROOT love VERB [I, quality, .]
the det quality NOUN []
quality dobj love VERB [the]
. punct love VERB []

Analyzing Feedback 2: 'The customer service was terrible, very disappointed.'
Tokens: ['The', 'customer', 'service', 'was', 'terrible', ',', 'very', 'disappointed', '.']
Lemmas: ['the', 'customer', 'service', 'be', 'terrible', ',', 'very', 'disappointed', '.']

Dependency Parsing:
The det service NOUN []
customer compound service NOUN []
service nsubj was AUX [The, customer]
was ROOT was AUX [service, disappointed, .]
terrible amod disappointed ADJ []
, punct disappointed ADJ []
very advmod disappointed ADJ []
disappointed acomp was AUX [terrible, ,, very]
. punct was AUX []

Analyzing Feedback 3: 'Great experience overall, highly recommended.'
Tokens: ['Great', 'experience', 'overall', ',', 'highly', 'recommended', '.']
Lemmas: ['great', 'experience', 'overall', ',', 'highly', 'recommend', '.']

Dependency Parsing:
Great amod experience NOUN []
experience nsubj recommended VERB [Great]
overall advmod recommended VERB []
, punct recommended VERB []
highly advmod recommended VERB []
recommended ROOT recommended VERB [experience, overall, ,, highly, .]
. punct recommended VERB []

Analyzing Feedback 4: 'The delivery was late, very frustrating.'
Tokens: ['The', 'delivery', 'was', 'late', ',', 'very', 'frustrating', '.']
Lemmas: ['the', 'delivery', 'be', 'late', ',', 'very', 'frustrating', '.']

Dependency Parsing:
The det delivery NOUN []
delivery nsubj was AUX [The]
was ROOT was AUX [delivery, frustrating, .]
late advmod frustrating ADJ []
, punct frustrating ADJ []
very advmod frustrating ADJ []
frustrating acomp was AUX [late, ,, very]
. punct was AUX []
 

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 3</h3>
    <pre>
        
import nltk
import random
nltk.download('punkt')
nltk.download('gutenberg')

words = nltk.corpus.gutenberg.words()

bigrams = list(nltk.bigrams(words))

starting_word = "the"
generated_text = [starting_word]

for _ in range(20):
    possible_words = [word2 for (word1, word2) in bigrams if word1.lower() ==
generated_text[-1].lower()]


    next_word = random.choice(possible_words)
    generated_text.append(next_word)

print(' '.join(generated_text))
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package gutenberg to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package gutenberg is already up-to-date!
the door of brass of the top of the niece came to behold , and overthrew the LORD , and Miss
 

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 4</h3>
    <pre>
        
!pip install transformers
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
class EmailAutocompleteSystem:
    def __init__(self):
        self.model_name = "gpt2"
        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)
        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)
    def generate_suggestions(self, user_input, context):
        input_text = f"{context} {user_input}"
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt")
        with torch.no_grad():
            output = self.model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)
            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
            suggestions = generated_text.split()[len(user_input.split()):]
        return suggestions

if __name__ == "__main__":
    autocomplete_system = EmailAutocompleteSystem()

    email_context = "Subject: Discussing Project Proposal\nHi [Recipient],"
while True:
    user_input = input("Enter your sentence (type 'exit' to end): ")
    if user_input.lower() == 'exit':
        break
    suggestions = autocomplete_system.generate_suggestions(user_input, email_context)
    if suggestions:
        print("Autocomplete Suggestions:", suggestions)
    else:
        print("No suggestions available.")
Requirement already satisfied: transformers in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (4.37.2)
Requirement already satisfied: filelock in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (3.13.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (0.20.3)
Requirement already satisfied: numpy>=1.17 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (1.23.4)
Requirement already satisfied: packaging>=20.0 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (21.3)
Requirement already satisfied: pyyaml>=5.1 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (6.0)
Requirement already satisfied: regex!=2019.12.17 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (2023.12.25)
Requirement already satisfied: requests in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (2.28.2)
Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (0.15.1)
Requirement already satisfied: safetensors>=0.4.1 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (0.4.2)
Requirement already satisfied: tqdm>=4.27 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from transformers) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from packaging>=20.0->transformers) (3.0.9)
Requirement already satisfied: colorama in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from tqdm>=4.27->transformers) (0.3.9)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from requests->transformers) (3.0.1)
Requirement already satisfied: idna<4,>=2.5 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from requests->transformers) (3.4)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from requests->transformers) (1.26.14)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from requests->transformers) (2022.12.7)
Enter your sentence (type 'exit' to end): Hi! How are you?
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Autocomplete Suggestions: ['Hi', '[Recipient],', 'Hi!', 'How', 'are', 'you?', "I'm", 'a', 'programmer', 'and', "I've", 'been', 'working', 'on', 'a', 'project', 'for', 'a', 'while', 'now.', 'I', 'have', 'a', 'few', 'ideas', 'for', 'the', 'project,', 'but', 'I', "don't", 'have']
Enter your sentence (type 'exit' to end): exit
 

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 5</h3>
    <pre>
        
!pip install scikit-learn

import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

categories = ['sci.med', 'sci.space', 'comp.graphics', 'talk.politics.mideast']
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)
newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)

X_train = newsgroups_train.data
X_test = newsgroups_test.data
y_train = newsgroups_train.target
y_test = newsgroups_test.target

model = make_pipeline(
 TfidfVectorizer(),
 LinearSVC()
)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)
print("\nClassification Report:")
print(classification_report(y_test, predictions))
Requirement already satisfied: scikit-learn in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (1.1.2)
Requirement already satisfied: numpy>=1.17.3 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from scikit-learn) (1.23.4)
Requirement already satisfied: scipy>=1.3.2 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from scikit-learn) (1.9.2)
Requirement already satisfied: joblib>=1.0.0 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from scikit-learn) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from scikit-learn) (3.1.0)
Accuracy: 0.9504823151125402

Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.97      0.93       389
           1       0.96      0.91      0.94       396
           2       0.98      0.94      0.96       394
           3       0.98      0.98      0.98       376

    accuracy                           0.95      1555
   macro avg       0.95      0.95      0.95      1555
weighted avg       0.95      0.95      0.95      1555

 

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 10</h3>
    <pre>
        
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
# Download NLTK resources (only required once)
nltk.download('vader_lexicon')
# Sample reviews
reviews = [
"This product is amazing! I love it.",
"The product was good, but the packaging was damaged.",
"Very disappointing experience. Would not recommend.",
"Neutral feedback on the product.",
]
# Initialize Sentiment Intensity Analyzer
sid = SentimentIntensityAnalyzer()
# Analyze sentiment for each review
for review in reviews:
    print("Review:", review)
    scores = sid.polarity_scores(review)
    print("Sentiment:", end=' ')
    if scores['compound'] > 0.05:
        print("Positive")
    elif scores['compound'] < -0.05:
        print("Negative")
    else:
        print("Neutral")
        print()
Review: This product is amazing! I love it.
Sentiment: Positive
Review: The product was good, but the packaging was damaged.
Sentiment: Negative
Review: Very disappointing experience. Would not recommend.
Sentiment: Negative
Review: Neutral feedback on the product.
Sentiment: Neutral

[nltk_data] Downloading package vader_lexicon to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package vader_lexicon is already up-to-date!
 

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 11</h3>
    <pre>
       
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
# Sample text for POS tagging
text = "Parts of speech tagging helps to understand the function of each word in a sentence."
# Tokenize the text into words
tokens = nltk.word_tokenize(text)
# Perform POS tagging
pos_tags = nltk.pos_tag(tokens)
# Display the POS tags
print("POS tags:", pos_tags)
POS tags: [('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('tagging', 'VBG'), ('helps', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('function', 'NN'), ('of', 'IN'), ('each', 'DT'), ('word', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')]
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
  

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 12</h3>
    <pre>
       
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
# Download NLTK resources (if not already downloaded)
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
def pos_tagging(text):
    sentences = sent_tokenize(text)
    tagged_tokens = []
    for sentence in sentences:
        tokens = word_tokenize(sentence)
        tagged_tokens.extend(nltk.pos_tag(tokens))
    return tagged_tokens
def main():
# Example news article
    article_text = """
    Manchester United secured a 3-1 victory over Chelsea in yesterday's
    match.
    Goals from Rashford, Greenwood, and Fernandes sealed the win for
    United.
    Chelsea's only goal came from Pulisic in the first half.
    The victory boosts United's chances in the Premier League title
    race.
    """
    tagged_tokens = pos_tagging(article_text)
    print("Original Article Text:\n", article_text)
    print("\nParts of Speech Tagging:")
    for token, pos_tag in tagged_tokens:
        print(f"{token}: {pos_tag}")
if __name__ == "__main__":
    main()
Original Article Text:
 
    Manchester United secured a 3-1 victory over Chelsea in yesterday's
    match.
    Goals from Rashford, Greenwood, and Fernandes sealed the win for
    United.
    Chelsea's only goal came from Pulisic in the first half.
    The victory boosts United's chances in the Premier League title
    race.
    

Parts of Speech Tagging:
Manchester: NNP
United: NNP
secured: VBD
a: DT
3-1: JJ
victory: NN
over: IN
Chelsea: NNP
in: IN
yesterday's: NNP
match: NN
.: .
Goals: NNS
from: IN
Rashford: NNP
,: ,
Greenwood: NNP
,: ,
and: CC
Fernandes: NNP
sealed: VBD
the: DT
win: NN
for: IN
United: NNP
.: .
Chelsea: NN
's: POS
only: JJ
goal: NN
came: VBD
from: IN
Pulisic: NNP
in: IN
the: DT
first: JJ
half: NN
.: .
The: DT
victory: NN
boosts: VBZ
United: NNP
's: POS
chances: NNS
in: IN
the: DT
Premier: NNP
League: NNP
title: NN
race: NN
.: .
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
  

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 13</h3>
    <pre>
        
import nltk
from nltk import RegexpParser
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
# Download NLTK resources (run only once if not downloaded)
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
# Sample sentence
sentence = "The quick brown fox jumps over the lazy dog"
# Tokenize the sentence
tokens = word_tokenize(sentence)
# POS tagging
tagged = pos_tag(tokens)
# Define a chunk grammar using regular expressions
# NP (noun phrase) chunking: "NP: {<DT>?<JJ>*<NN>}"
# This grammar captures optional determiner (DT), adjectives (JJ), and nouns (NN) as a noun phrase
chunk_grammar = r"""
NP: {<DT>?<JJ>*<NN>}
"""
# Create a chunk parser with the defined grammar
chunk_parser = RegexpParser(chunk_grammar)

# Parse the tagged sentence to extract chunks
chunks = chunk_parser.parse(tagged)
# Display the chunks
for subtree in chunks.subtrees():
    if subtree.label() == 'NP': # Print only noun phrases
        print(subtree)
(NP The/DT quick/JJ brown/NN)
(NP fox/NN)
(NP the/DT lazy/JJ dog/NN)
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
 
        

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 14</h3>
    <pre>
        
import nltk
import os
# Set NLTK data path
nltk.data.path.append("/usr/local/share/nltk_data")
# Download the 'punkt' tokenizer model
nltk.download('punkt')
# Download the 'averaged_perceptron_tagger' model
nltk.download('averaged_perceptron_tagger')
# Sample text
text = "The quick brown fox jumps over the lazy dog."
# Tokenize the text into words
words = nltk.word_tokenize(text)
# Perform part-of-speech tagging
pos_tags = nltk.pos_tag(words)
# Define chunk grammar
chunk_grammar = r"""
NP: {<DT>?<JJ>*<NN>} # Chunk sequences of DT, JJ, NN
"""
# Create chunk parser
chunk_parser = nltk.RegexpParser(chunk_grammar)
# Apply chunking
chunked_text = chunk_parser.parse(pos_tags)
# Extract noun phrases
noun_phrases = []
for subtree in chunked_text.subtrees(filter=lambda t: t.label() =='NP'):
    noun_phrases.append(' '.join(word for word, tag in
    subtree.leaves()))
    # Output
    print("Original Text:", text)
    print("Noun Phrases:")
    for phrase in noun_phrases:
        print("-", phrase)
Original Text: The quick brown fox jumps over the lazy dog.
Noun Phrases:
- The quick brown
Original Text: The quick brown fox jumps over the lazy dog.
Noun Phrases:
- The quick brown
- fox
Original Text: The quick brown fox jumps over the lazy dog.
Noun Phrases:
- The quick brown
- fox
- the lazy dog
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
 

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 6</h3>
    <pre>
        
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report

newsgroups = fetch_20newsgroups(subset='all', categories=['comp.sys.ibm.pc.hardware',
'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles', 'sci.electronics'])

X = newsgroups.data
y = newsgroups.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

classifier = LinearSVC()
classifier.fit(X_train, y_train)

predictions = classifier.predict(X_test)

accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)
print("\nClassification Report:")
print(classification_report(y_test, predictions, target_names=newsgroups.target_names))
Accuracy: 0.9389623601220752

Classification Report:
                          precision    recall  f1-score   support

comp.sys.ibm.pc.hardware       0.92      0.91      0.91       212
   comp.sys.mac.hardware       0.94      0.93      0.94       198
               rec.autos       0.97      0.93      0.95       179
         rec.motorcycles       0.96      0.99      0.97       205
         sci.electronics       0.92      0.93      0.92       189

                accuracy                           0.94       983
               macro avg       0.94      0.94      0.94       983
            weighted avg       0.94      0.94      0.94       983

 

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 7</h3>
    <pre>
        
!pip install gensim
!pip install nltk
# Import required libraries
import gensim.downloader as api
from nltk.tokenize import word_tokenize
# Download pre-trained word vectors (Word2Vec)
word_vectors = api.load("word2vec-google-news-300")
# Sample sentences
sentences = [
"Natural language processing is a challenging but fascinating field.",
"Word embeddings capture semantic meanings of words in a vector space."
]

# Tokenize sentences
tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]
# Perform semantic analysis using pre-trained word vectors
for tokenized_sentence in tokenized_sentences:
    for word in tokenized_sentence:
        if word in word_vectors:
            similar_words = word_vectors.most_similar(word)
            print(f"Words similar to '{word}': {similar_words}")
        else:
            print(f"'{word}' is not in the pre-trained Word2Vec model.")
Requirement already satisfied: gensim in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (4.3.2)
Requirement already satisfied: numpy>=1.18.5 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from gensim) (1.23.4)
Requirement already satisfied: scipy>=1.7.0 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from gensim) (1.9.2)
Requirement already satisfied: smart-open>=1.8.1 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from gensim) (6.4.0)
Requirement already satisfied: nltk in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (3.8.1)
Requirement already satisfied: click in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from nltk) (8.1.3)
Requirement already satisfied: joblib in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from nltk) (1.2.0)
Requirement already satisfied: regex>=2021.8.3 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from nltk) (2023.12.25)
Requirement already satisfied: tqdm in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from nltk) (4.65.0)
Requirement already satisfied: colorama in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from click->nltk) (0.3.9)
[=-------------------------------------------------] 2.4% 39.9/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[=======-------------------------------------------] 14.0% 233.3/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[==========----------------------------------------] 20.7% 343.6/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[==========----------------------------------------] 21.9% 364.8/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[===========---------------------------------------] 22.8% 378.6/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[============--------------------------------------] 25.7% 428.1/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[=====================-----------------------------] 43.8% 728.9/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[===========================-----------------------] 55.0% 914.6/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[============================----------------------] 56.1% 932.9/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[============================----------------------] 57.6% 957.4/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[================================------------------] 64.4% 1070.6/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[=================================-----------------] 66.5% 1105.6/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[===================================---------------] 70.6% 1173.7/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[===================================---------------] 71.7% 1191.8/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[====================================--------------] 73.7% 1225.3/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[=====================================-------------] 75.3% 1251.4/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[======================================------------] 77.7% 1292.7/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[========================================----------] 80.5% 1339.1/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[============================================------] 88.2% 1466.6/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[============================================------] 89.7% 1492.0/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[=============================================-----] 90.8% 1509.3/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[==============================================----] 92.6% 1539.4/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[==============================================----] 93.5% 1554.6/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[===============================================---] 95.3% 1585.0/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[================================================--] 96.3% 1600.9/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[=================================================-] 98.1% 1632.0/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

[=================================================-] 99.8% 1659.2/1662.8MB downloaded
IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

Words similar to 'natural': [('Splittorff_lacked', 0.636509358882904), ('Natural', 0.58078932762146), ('Mike_Taugher_covers', 0.577259361743927), ('manmade', 0.5276211500167847), ('shell_salted_pistachios', 0.5084421634674072), ('unnatural', 0.5030758380889893), ('naturally', 0.49992606043815613), ('Intraparty_squabbles', 0.4988228678703308), ('Burt_Bees_¬Æ', 0.49539363384246826), ('causes_Buxeda', 0.4935200810432434)]
Words similar to 'language': [('langauge', 0.7476695775985718), ('Language', 0.6695356369018555), ('languages', 0.6341332197189331), ('English', 0.6120712757110596), ('CMPB_Spanish', 0.6083104610443115), ('nonnative_speakers', 0.6063109636306763), ('idiomatic_expressions', 0.5889801979064941), ('verb_tenses', 0.58415687084198), ('Kumeyaay_Diegueno', 0.5798824429512024), ('dialect', 0.5724600553512573)]
Words similar to 'processing': [('Processing', 0.7285515666007996), ('processed', 0.6519132852554321), ('processor', 0.636760413646698), ('warden_Dominick_DeRose', 0.6166526675224304), ('processors', 0.5953895449638367), ('Discoverer_Enterprise_resumed', 0.5376213192939758), ('LSI_Tarari', 0.520267903804779), ('processer', 0.5166687369346619), ('remittance_processing', 0.5144169926643372), ('Farmland_Foods_pork', 0.5071728825569153)]
Words similar to 'is': [('was', 0.6549733281135559), ("isn'ta", 0.6439523100852966), ('seems', 0.634029746055603), ('Is', 0.6085968613624573), ('becomes', 0.5841935276985168), ('appears', 0.5822900533676147), ('remains', 0.5796942114830017), ('—ñ—ï', 0.5695518255233765), ('makes', 0.5567088723182678), ('isn_`_t', 0.5513144135475159)]
'a' is not in the pre-trained Word2Vec model.
Words similar to 'challenging': [('difficult', 0.6388775110244751), ('challenge', 0.5953003764152527), ('daunting', 0.569800615310669), ('tough', 0.5689979791641235), ('challenges', 0.5471934676170349), ('challenged', 0.5449535846710205), ('Challenging', 0.5242965817451477), ('tricky', 0.5236554741859436), ('toughest', 0.5169045329093933), ('diffi_cult', 0.5010539889335632)]
Words similar to 'but': [('although', 0.8104525804519653), ('though', 0.7285684943199158), ('because', 0.7225914597511292), ('so', 0.6865807771682739), ('But', 0.6826984882354736), ('Although', 0.6188263297080994), ('Though', 0.6153667569160461), ('Unfortunately', 0.6031029224395752), ('Of_course', 0.593142032623291), ('anyway', 0.5869061350822449)]
Words similar to 'fascinating': [('interesting', 0.7623067498207092), ('intriguing', 0.7245113253593445), ('enlightening', 0.6644250154495239), ('captivating', 0.6459898352622986), ('facinating', 0.6416683793067932), ('riveting', 0.6324825286865234), ('instructive', 0.6210989356040955), ('endlessly_fascinating', 0.6188612580299377), ('revelatory', 0.6170244216918945), ('engrossing', 0.6126049160957336)]
Words similar to 'field': [('fields', 0.5582526326179504), ('fi_eld', 0.5188260078430176), ('Keith_Toogood', 0.49749255180358887), ('Mackenzie_Hoambrecker', 0.49514278769493103), ('Josh_Arauco_kicked', 0.48817265033721924), ('Nick_Cattoi', 0.4863145053386688), ('Armando_Cuko', 0.4853871166706085), ('Jon_Striefsky', 0.48322004079818726), ('kicker_Nico_Grasu', 0.47572532296180725), ('Chris_Manfredini_kicked', 0.47327715158462524)]
'.' is not in the pre-trained Word2Vec model.
Words similar to 'word': [('phrase', 0.6777030825614929), ('words', 0.5864380598068237), ('verb', 0.5517287254333496), ('Word', 0.54575115442276), ('adjective', 0.5290762186050415), ('cuss_word', 0.5272089242935181), ('colloquialism', 0.5160348415374756), ('noun', 0.5129537582397461), ('astrology_#/##/##', 0.5039082765579224), ('synonym', 0.49379870295524597)]
'embeddings' is not in the pre-trained Word2Vec model.
Words similar to 'capture': [('capturing', 0.7563897371292114), ('captured', 0.7155306935310364), ('captures', 0.6099075078964233), ('Capturing', 0.6023245453834534), ('recapture', 0.5498639941215515), ('Capture', 0.5493018627166748), ('nab', 0.4941576421260834), ('Captured', 0.45745959877967834), ('apprehend', 0.4357919692993164), ('seize', 0.4338296055793762)]
Words similar to 'semantic': [('semantics', 0.6644964814186096), ('Semantic', 0.6464474201202393), ('contextual', 0.5909127593040466), ('meta', 0.5905876755714417), ('ontology', 0.5880525708198547), ('Semantic_Web', 0.5612248778343201), ('semantically', 0.5600483417510986), ('microformat', 0.5582399368286133), ('inferencing', 0.5541478991508484), ('terminological', 0.5533202290534973)]
Words similar to 'meanings': [('grammatical_constructions', 0.594986081123352), ('idioms', 0.5938195586204529), ('connotations', 0.5836683511734009), ('symbolic_meanings', 0.5806494951248169), ('meaning', 0.5785343647003174), ('literal_meanings', 0.5743482112884521), ('denotative', 0.5730364918708801), ('phrasal_verbs', 0.5697917342185974), ('contexts', 0.5609514713287354), ('adjectives_adverbs', 0.5569407343864441)]
'of' is not in the pre-trained Word2Vec model.
Words similar to 'words': [('phrases', 0.7100036144256592), ('phrase', 0.6408688426017761), ('Words', 0.6160537600517273), ('word', 0.5864380598068237), ('adjectives', 0.5812757015228271), ('uttered', 0.5724518299102783), ('plate_umpire_Tony_Randozzo', 0.5642045140266418), ('expletives', 0.5539036989212036), ('Mayor_Cirilo_Pena', 0.553884744644165), ('Tele_prompter', 0.5441114902496338)]
Words similar to 'in': [('inthe', 0.5891957879066467), ('where', 0.5662435293197632), ('the', 0.5429296493530273), ('In', 0.5415117144584656), ('during', 0.5188906192779541), ('iin', 0.48737412691116333), ('at', 0.484235554933548), ('from', 0.48268404603004456), ('outside', 0.47092658281326294), ('for', 0.4566476047039032)]
'a' is not in the pre-trained Word2Vec model.
Words similar to 'vector': [('vectors', 0.750322163105011), ('adeno_associated_viral_AAV', 0.5999537110328674), ('bitmap_graphics', 0.5428463220596313), ('Sindbis', 0.5353653430938721), ('bitmap_images', 0.5318013429641724), ('signal_analyzer_VSA', 0.5276671051979065), ('analyzer_VNA', 0.5184376239776611), ('vectorial', 0.5084835886955261), ('nonviral_gene_therapy', 0.5036363005638123), ('shellcode', 0.5015827417373657)]
Words similar to 'space': [('spaces', 0.6570690870285034), ('music_concept_ShockHound', 0.5850345492362976), ('Shuttle_docks', 0.5566749572753906), ('Space', 0.5478203296661377), ('Soviet_Union_Yuri_Gagarin', 0.5417766571044922), ('Shuttle_Discovery_blasts', 0.5352603197097778), ('Shuttle_Discovery_docks', 0.534925103187561), ('Shuttle_Endeavour_undocks', 0.532420814037323), ('Shuttle_Discovery_arrives', 0.5323426723480225), ('Shuttle_undocks', 0.523307740688324)]
'.' is not in the pre-trained Word2Vec model.
 

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 8</h3>
    <pre>
        
import nltk
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
# Initialize NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
# Function to perform semantic analysis
def semantic_analysis(text):
# Tokenize text
    tokens = word_tokenize(text)
        # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
    # Synonyms generation
    synonyms = set()
    for token in lemmatized_tokens:
        for syn in wordnet.synsets(token):
            for lemma in syn.lemmas():
                synonyms.add(lemma.name())
    return list(synonyms)
# Example customer queries
customer_queries = [
"I received a damaged product. Can I get a refund?",
"I'm having trouble accessing my account.",
"How can I track my order status?",
"The item I received doesn't match the description.",
"Is there a discount available for bulk orders?"
]
# Semantic analysis for each query
for query in customer_queries:
    print("Customer Query:", query)
    synonyms = semantic_analysis(query)
    print("Semantic Analysis (Synonyms):", synonyms)
    print("\n")
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Customer Query: I received a damaged product. Can I get a refund?
Semantic Analysis (Synonyms): ['pay_back', 'pose', 'sustain', 'start', 'mathematical_product', 'discredited', 'repay', 'stick', 'start_out', 'drive', 'contract', 'perplex', 'pay_off', 'repayment', 'bring_forth', 'merchandise', 'bewilder', 'puzzle', 'generate', 'aim', 'product', 'dumbfound', 'Cartesian_product', 'obtain', 'produce', 'received', 'draw', 'take_in', 'fetch', 'cause', 'scram', 'fuck_off', 'get', 'mystify', 'induce', 'return', 'find', 'arrest', "get_under_one's_skin", 'amaze', 'acquire', 'sire', 'have', 'become', 'flummox', 'set_out', 'damage', 'bring', 'take', 'engender', 'give_back', 'develop', 'fix', 'production', 'begin', 'suffer', 'convey', 'beat', 'grow', 'stimulate', 'come', 'set_about', 'standard', 'make', 'stupefy', 'experience', 'let', 'bugger_off', 'commence', 'catch', 'meet', 'encounter', 'capture', 'nonplus', 'receive', 'buzz_off', 'mother', 'damaged', 'intersection', 'refund', 'gravel', 'beget', 'go', 'ware', 'arrive', 'baffle', 'father', 'welcome', 'pick_up', 'incur', 'get_down', 'vex', 'invite']


Customer Query: I'm having trouble accessing my account.
Semantic Analysis (Synonyms): ['inconvenience_oneself', 'chronicle', 'difficulty', 'inconvenience', 'bill', 'trouble', 'report', 'score', 'pain', 'access', 'invoice', 'explanation', 'business_relationship', 'answer_for', 'news_report', 'calculate', 'story', 'account_statement', 'describe', 'perturb', 'upset', 'disquiet', 'put_out', 'accounting', 'distract', 'discommode', 'history', 'get_at', 'worry', 'disorder', 'incommode', 'fuss', 'write_up', 'disturb', 'ail', 'problem', 'bother', 'unhinge', 'hassle', 'account', 'disoblige', 'trouble_oneself', 'cark']


Customer Query: How can I track my order status?
Semantic Analysis (Synonyms): ['monastic_order', 'trail', 'cut_through', 'ordain', 'path', 'Holy_Order', 'say', 'parliamentary_law', 'dog', 'club', 'rail', 'decree', 'go_after', 'range', 'data_track', 'lead', 'rescript', 'dictate', 'tag', 'give_chase', 'govern', 'get_over', 'condition', 'Order', 'rate', 'edict', 'regularize', 'consecrate', 'guild', 'tail', 'course', 'order_of_magnitude', 'social_club', 'parliamentary_procedure', 'cart_track', 'chase_after', 'rank', 'cartroad', 'purchase_order', 'ordinate', 'fiat', 'rules_of_order', 'pass_over', 'traverse', 'chase', 'rails', 'get_across', 'set_up', 'raceway', 'racetrack', 'racecourse', 'society', 'caterpillar_track', 'orderliness', 'ordering', 'prescribe', 'status', 'regulate', 'cover', 'regularise', 'lodge', 'order', 'caterpillar_tread', 'runway', 'cut_across', 'position', 'running', 'tell', 'grade', 'gild', 'ordination', 'place', 'cut', 'put', 'enjoin', 'cross', 'track', 'arrange']


Customer Query: The item I received doesn't match the description.
Semantic Analysis (Synonyms): ['point', 'encounter', 'mate', 'couple', 'mates', 'item', 'peer', 'fit', 'play_off', 'agree', 'match', 'take_in', 'receive', 'compeer', 'oppose', 'equalise', 'token', 'cope_with', 'twin', 'description', 'verbal_description', 'pit', 'equalize', 'equal', 'friction_match', 'correspond', 'get', 'lucifer', 'jibe', 'detail', 'pair', 'find', 'gibe', 'touch', 'standard', 'check', 'have', 'experience', 'tally', 'rival', 'welcome', 'particular', 'pick_up', 'obtain', 'catch', 'incur', 'invite', 'meet', 'equate', 'received']


Customer Query: Is there a discount available for bulk orders?
Semantic Analysis (Synonyms): ['Order', 'bank_discount', 'price_reduction', 'discount', 'monastic_order', 'rate', 'ignore', 'deduction', 'order', 'edict', 'regularize', 'consecrate', 'guild', 'available', 'order_of_magnitude', 'social_club', 'ordain', 'parliamentary_procedure', 'society', 'brush_off', 'Holy_Order', 'say', 'bulk', 'rebate', 'discount_rate', 'volume', 'orderliness', 'parliamentary_law', 'ordering', 'club', 'tell', 'grade', 'rank', 'gild', 'ordination', 'prescribe', 'purchase_order', 'push_aside', 'majority', 'decree', 'ordinate', 'range', 'fiat', 'mass', 'regulate', 'rules_of_order', 'place', 'put', 'bulge', 'brush_aside', 'rescript', 'enjoin', 'dictate', 'uncommitted', 'regularise', 'lodge', 'govern', 'useable', 'usable', 'arrange', 'dismiss', 'set_up', 'disregard']


 

        
    </pre>
</div><br/><hr/>
<div class="codes">
    <h3>Exp 9</h3>
    <pre>
        
!pip install scikit-learn
!pip install nltk
# Import required libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from nltk.corpus import movie_reviews # Sample dataset from NLTK
# Download NLTK resources (run only once if not downloaded)
import nltk
nltk.download('movie_reviews')
# Load the movie_reviews dataset
documents = [(list(movie_reviews.words(fileid)), category)
for category in movie_reviews.categories()
for fileid in movie_reviews.fileids(category)]
# Convert data to DataFrame
df = pd.DataFrame(documents, columns=['text', 'sentiment'])
# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2,
random_state=42)
# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()
# Fit and transform the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train.apply(' '.join))
# Initialize SVM classifier
svm_classifier = SVC(kernel='linear')
# Train the classifier
svm_classifier.fit(X_train_tfidf, y_train)
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test.apply(' '.join))
# Predict on the test data
y_pred = svm_classifier.predict(X_test_tfidf)
# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
# Display classification report
print(classification_report(y_test, y_pred))
Requirement already satisfied: scikit-learn in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (1.1.2)
Requirement already satisfied: numpy>=1.17.3 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from scikit-learn) (1.23.4)
Requirement already satisfied: scipy>=1.3.2 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from scikit-learn) (1.9.2)
Requirement already satisfied: joblib>=1.0.0 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from scikit-learn) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from scikit-learn) (3.1.0)
Requirement already satisfied: nltk in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (3.8.1)
Requirement already satisfied: click in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from nltk) (8.1.3)
Requirement already satisfied: joblib in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from nltk) (1.2.0)
Requirement already satisfied: regex>=2021.8.3 in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from nltk) (2023.12.25)
Requirement already satisfied: tqdm in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from nltk) (4.65.0)
Requirement already satisfied: colorama in c:\users\online\appdata\local\programs\python\python310\lib\site-packages (from click->nltk) (0.3.9)
[nltk_data] Downloading package movie_reviews to
[nltk_data]     C:\Users\online\AppData\Roaming\nltk_data...
[nltk_data]   Package movie_reviews is already up-to-date!
Accuracy: 0.84
              precision    recall  f1-score   support

         neg       0.83      0.85      0.84       199
         pos       0.85      0.82      0.84       201

    accuracy                           0.84       400
   macro avg       0.84      0.84      0.84       400
weighted avg       0.84      0.84      0.84       400

 

        
    </pre>
</div><br/><hr/>




</div>

<div class="datasets">
    <h3 style="text-decoration:overline;">Datasets üìÇ</h3>
    <div class="ds1">
        <button id="btn_down1" onclick="downloadFile('abcde.csv')">
            abcde üì©
        </button>
    </div>
    <div class="ds2">
        <button id="btn_down2" onclick="downloadFile('diabetes.csv')">
            diabetes üì©
        </button>
    </div>
    <div class="ds3">
        <button id="btn_down3" onclick="downloadFile('iris.csv')">
            iris üì©
        </button>
    </div>
    <div class="ds4">
        <button id="btn_down4" onclick="downloadFile('Social_Network_Ads.csv')">
            Social Network Ads üì©
        </button>
    </div>
    <div class="ds5"></div>
    <button id="btn_down5" onclick="downloadFile('Dataset_exp1.csv')">
       Dataset Exp1 üì©
    </button>
    <div class="ds6">
        <button id="btn_down4" onclick="downloadFile('decision_tree.csv')">
            Decision Tree üì©
        </button>
    </div>
</div>

<script>
    function downloadFile(filename) {
        var anchor = document.createElement('a');
        anchor.setAttribute('href', filename);
        anchor.setAttribute('download', filename);
        anchor.click();
    }
</script>

</body>
</html>
