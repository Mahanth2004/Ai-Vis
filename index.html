<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-Visualization Lab-Web</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <h1>
        <!-- Lab Experiments: -->
    </h1>
    
<div class="exp_code">
    <div class="codes">
        <h3>Exp 1- Techniques for data preprocessing</h3>
        <pre>
            <b style="color: green;">A.</b>
            .import csv
import statistics
def remove_mean(input_file,output_file):
    with open(input_file,'r') as file:
        reader=csv.reader(file)
        data=[float(row[0]) for row in reader]
    mean_value=statistics.mean(data)
    mean_removed_data=[x-mean_value for x in data]
    with open(output_file,'w',newline="")as file:
        writer=csv.writer(file)
        for value in mean_removed_data:
            writer.writerow([value])
if __name__=="__main__":
    input_file="data.csv"
    output_file="mean_removed_data"
    remove_mean(input_file,output_file)

    <b style="color: green;">B.</b>
    .import csv
    import statistics
    def remove_mean(input_file,output_file):
        with open(input_file,'r') as file:
            reader=csv.reader(file)
            data =[float(row[0]) for row in reader]
        mean_value=statistics.mean(data)
        mean_removed_data=[x-mean_value for x in data]
        with open (output_file,'w',newline='')as file:
            writer=csv.writer(file)
            for value in mean_removed_data:
                writer.writerow([value])
    if __name__ =="__main__":
        input_file=r"data.csv"
        output_file="mean_removed_data.csv"
        
        <b style="color: green;">C.</b>
    import csv
import statistics
def z_score_normalization(input_file,output_file):
    with open(input_file,'r') as file:
        reader =csv.reader(file)
        data=[float(row[0]) for row in reader]
    mean_value=statistics.mean(data)
    standard_dev=statistics.stdev(data)
    z_scored_data=[(x-mean_value)/standard_dev for x in data]                                                                          
    with open(output_file,'w',newline="") as file:
        writer = csv.writer(file)
        for value in z_scored_data:
            writer.writerow([value])
if __name__ == "__main__":
    input_file="data.csv"
    output_file = "z_normalization_data.csv"
    z_score_normalization(input_file,output_file)
        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2a -  2.a.  Na√Øve Bayes Classifier</h3>
        <pre>
           
import pandas as pd
f = pd.DataFrame({'Weather':['Sunny', 'Rainy', 'Sunny', 'Sunny'],
                'Wind':['Mild', 'Mild', 'High', 'Mild'],
                'Temp':['Moderate', 'Mild', 'Moderate', 'Mild'],
                'go':['Yes', 'No', 'Yes', 'Yes']})
print(f.columns)

from sklearn.naive_bayes import GaussianNB as g
from sklearn.preprocessing import LabelEncoder as le
from sklearn.model_selection import train_test_split as tt

l = le()
for i in f.columns:
    f[i] = l.fit_transform(f[i])
    x = f.iloc[:, :3]
    y = f.iloc[:, 3]

xtr, xte, ytr, yte = tt(x, y, test_size=0.3)
gg = g()
gg.fit(xtr, ytr)
y_pred = gg.predict(xte)

from sklearn.metrics import accuracy_score
print(accuracy_score(yte, y_pred))

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2b - 2.b.Support vector machine</h3>
        <pre>
            import numpy as np
            import matplotlib.pyplot as plt
            import pandas as pd
            dataset = pd.read_csv("Social_Network _Ads1.csv")
            X = dataset.iloc[:, [2, 3]].values 
            y = dataset.iloc[:, 4].values
            from sklearn.model_selection import train_test_split 
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
            from sklearn.preprocessing import StandardScaler
            sc = StandardScaler()
            X_train = sc.fit_transform(X_train)
            X_test = sc.transform(X_test)
            from sklearn.svm import SVC
            classifier = SVC(kernel='rbf', random_state = 0)
            classifier.fit(X_train, y_train)
            y_pred = classifier.predict(X_test)
            from sklearn.metrics import confusion_matrix, accuracy_score
            cm = confusion_matrix(y_test, y_pred)
            print(cm)
            accuracy_score(y_test,y_pred)
            from matplotlib.colors import ListedColormap
            X_set, y_set = X_test, y_test
            X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
            plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),alpha = 0.75, cmap = ListedColormap(('red', 'green')))
            plt.xlim(X1.min(), X1.max())
            plt.ylim(X2.min(), X2.max())
            for i, j in enumerate(np.unique(y_set)):
                plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],c = ListedColormap(('pink', 'green'))(i), label = j)
            plt.title('SVM (Test set)')
            plt.xlabel('Age')
            plt.ylabel('Estimated Salary')
            plt.legend()
            plt.show()
            
        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2c - 2.c.Logistic regression</h3>
        <pre>
            import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from matplotlib.colors import ListedColormap

dataset = pd.read_csv("diabetes1.csv")

x = dataset.iloc[:, [4, 7]].values
y = dataset.iloc[:, 8].values

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25, random_state = 0)

sc_x = StandardScaler()
xtrain = sc_x.fit_transform(xtrain)
xtest = sc_x.transform(xtest)

print (xtrain[0:10, :])

classifier = LogisticRegression(random_state = 0)
classifier.fit(xtrain, ytrain)
y_pred = classifier.predict(xtest)

cm = confusion_matrix(ytest, y_pred)

print ("Confusion Matrix : \n", cm)

print ("Accuracy : ", accuracy_score(ytest, y_pred))

X_set, y_set = xtest, ytest
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, 
          stop = X_set[:, 0].max() + 1, step = 0.01),
          np.arange(start = X_set[:, 1].min() - 1,
          stop = X_set[:, 1].max() + 1, step = 0.01))

plt.contourf(X1, X2, classifier.predict(
      np.array([X1.ravel(), X2.ravel()]).T).reshape(
      X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))

plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())

for i, j in enumerate(np.unique(y_set)):
  plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
        c = ListedColormap(('red', 'green'))(i), label = j)
  
plt.title('Classifier (Test set)')
plt.xlabel('Age')
plt.ylabel('Diabetes')
plt.legend()
plt.show()

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2d - 2.d.DECISION TREE</h3>
        <pre>
            import pandas as pd
import math
import numpy as np

data = pd.read_csv("3-dataset.csv")
features = [feat for feat in data]
features.remove("answer")

class Node:
    def __init__(self):
        self.children = []
        self.value = ""
        self.isLeaf = False
        self.pred = ""
def entropy(examples):
    pos = 0.0
    neg = 0.0
    for _, row in examples.iterrows():
        if row["answer"] == "yes":
            pos += 1
        else:
            neg += 1
    if pos == 0.0 or neg == 0.0:
        return 0.0
    else:
        p = pos / (pos + neg)
        n = neg / (pos + neg)
        return -(p * math.log(p, 2) + n * math.log(n, 2))

    
def info_gain(examples, attr):
    uniq = np.unique(examples[attr])
    gain = entropy(examples)
    for u in uniq:
        subdata = examples[examples[attr] == u]
        sub_e = entropy(subdata)
        gain -= (float(len(subdata)) / float(len(examples))) * sub_e
    return gain

def ID3(examples, attrs):
    root = Node()

    max_gain = 0
    max_feat = ""
    for feature in attrs:
        gain = info_gain(examples, feature)
        if gain > max_gain:
            max_gain = gain
            max_feat = feature
    root.value = max_feat
    uniq = np.unique(examples[max_feat])
    for u in uniq:
        subdata = examples[examples[max_feat] == u]
        if entropy(subdata) == 0.0:
            newNode = Node()
            newNode.isLeaf = True
            newNode.value = u
            newNode.pred = np.unique(subdata["answer"])
            root.children.append(newNode)
        else:
            dummyNode = Node()
            dummyNode.value = u
            new_attrs = attrs.copy()
            new_attrs.remove(max_feat)
            child = ID3(subdata, new_attrs)
            dummyNode.children.append(child)
            root.children.append(dummyNode)

    return root

def printTree(root: Node, depth=0):
    for i in range(depth):
        print("\t", end="")
    print(root.value, end="")
    if root.isLeaf:
        print(" -> ", root.pred)
    print()
    for child in root.children:
        printTree(child, depth + 1)

def classify(root: Node, new):
    for child in root.children:
        if child.value == new[root.value]:
            if child.isLeaf:
                print ("Predicted Label for new example", new," is:", child.pred)
                exit
            else:
                classify (child.children[0], new)

root = ID3(data, features)
print("Decision Tree is:")
printTree(root)
print ("------------------")

new = {"outlook":"sunny", "temperature":"hot", "humidity":"normal", "wind":"strong"}
classify (root, new)                

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 2e - 2.e.Random forest</h3>
        <pre>
            import pandas as pd
data=pd.read_csv("HeartDisease1.csv")
X =data.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12]].values
y =data.iloc[:,13].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier()
rfc.fit(X_train, y_train)
y_pred=rfc.predict(X_test)
from sklearn import metrics
print("Classification Accuracy:", metrics.accuracy_score(y_test, y_pred)*100)
cm=metrics.confusion_matrix(y_test,y_pred)
print(cm)
import seaborn as sn
from matplotlib import pyplot as plt
plt.figure(figsize=(5,4))
sn.heatmap(cm,annot=True)
plt.xlabel('Predicted value')
plt.ylabel('Actual value')
plt.show()

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 3 - K-MEANS</h3>
        <pre>
            import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.cluster import KMeans 
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler
iris = pd.read_csv("Iris1.csv")
x = iris.iloc[:, [ 1,2,3,4]].values

from sklearn.cluster import KMeans
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)


kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(x)
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'blue', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'orange', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')

plt.legend()

plt.show()

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 5</h3>
        <pre>
            import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist

nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_words = [word.lower() for word in word_tokens if word.isalpha() and word.lower() not in stop_words]
    return filtered_words

def create_bow_model(texts):
    all_words = []
    for text in texts:
        words = preprocess_text(text)
        all_words.extend(words)

    word_freq = FreqDist(all_words)
    bow_model = {word: freq for word, freq in word_freq.items()}
    return bow_model

# Example usage
texts = [
    "The cat sat on the mat, and the mat was comfortable.",
    "She sang a sweet song, a song that touched everyone's heart.",
    "Coding coding can be challenging, but coding is also incredibly rewarding.",
]

bow_model = create_bow_model(texts)

# Print the Bag of Words model
print("Bag of Words Model:")
for word, freq in bow_model.items():
    print(f"{word}: {freq}")

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 6</h3>
       <pre>
        import csv
        import re
        def identify_patterns(csv_file_path, column_name):
            patterns = {}
            with open(csv_file_path, 'r') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    text = row[column_name]
        
                    pattern_matches = re.findall(r'Female', text, flags=re.IGNORECASE)
        
                    for match in pattern_matches:
                        if match in patterns:
                            patterns[match] += 1
                        else:
                            patterns[match] = 1
            return patterns
        csv_file_path = 'Social_Network _Ads.csv'
        column_name = 'Gender'    
        result = identify_patterns(csv_file_path, column_name)
        for pattern, count in result.items():
            print(f"Pattern: {pattern}, Count: {count}")
       </pre>



    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 7</h3>
       <pre>
       import gensim.downloader as api
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

word_vectors = api.load("glove-wiki-gigaword-100")
sentences = [
    "Natural language processing is a challenging but fascinating field.",
    "Word embeddings capture semantic meanings of words in a vector space."
]

tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]

for tokenized_sentence in tokenized_sentences:
    for word in tokenized_sentence:
        if word in word_vectors:
            similar_words = word_vectors.most_similar(word)
            print(f"Words similar to '{word}': {similar_words}")
        else:
            print(f"'{word}' is not in the pre-trained Word2Vec model.")
       </pre>

    </div><br/><hr/>
    

    <div class="codes">
        <h3>Exp 8</h3>
        <pre>
            def aStarAlgo(start_node, stop_node):
         
        open_set = set(start_node) 
        closed_set = set()
        g = {} #store distance from starting node
        parents = {}# parents contains an adjacency map of all nodes
 
        #ditance of starting node from itself is zero
        g[start_node] = 0
        #start_node is root node i.e it has no parent nodes
        #so start_node is set to its own parent node
        parents[start_node] = start_node
         
         
        while len(open_set) > 0:
            n = None
 
            #node with lowest f() is found
            for v in open_set:
                if n == None or g[v] + heuristic(v) < g[n] + heuristic(n):
                    n = v
             
                     
            if n == stop_node or Graph_nodes[n] == None:
                pass
            else:
                for (m, weight) in get_neighbors(n):
                    #nodes 'm' not in first and last set are added to first
                    #n is set its parent
                    if m not in open_set and m not in closed_set:
                        open_set.add(m)
                        parents[m] = n
                        g[m] = g[n] + weight
                         
     
                    #for each node m,compare its distance from start i.e g(m) to the
                    #from start through n node
                    else:
                        if g[m] > g[n] + weight:
                            #update g(m)
                            g[m] = g[n] + weight
                            #change parent of m to n
                            parents[m] = n
                             
                            #if m in closed set,remove and add to open
                            if m in closed_set:
                                closed_set.remove(m)
                                open_set.add(m)
 
            if n == None:
                print('Path does not exist!')
                return None
 
            # if the current node is the stop_node
            # then we begin reconstructin the path from it to the start_node
            if n == stop_node:
                path = []
 
                while parents[n] != n:
                    path.append(n)
                    n = parents[n]
 
                path.append(start_node)
 
                path.reverse()
 
                print('Path found: {}'.format(path))
                return path
 
 
            # remove n from the open_list, and add it to closed_list
            # because all of his neighbors were inspected
            open_set.remove(n)
            closed_set.add(n)
 
        print('Path does not exist!')
        return None
         
#define fuction to return neighbor and its distance
#from the passed node
def get_neighbors(v):
    if v in Graph_nodes:
        return Graph_nodes[v]
    else:
        return None
#for simplicity we ll consider heuristic distances given
#and this function returns heuristic distance for all nodes
def heuristic(n):
        H_dist = {
            'A': 11,
            'B': 6,
            'C': 99,
            'D': 1,
            'E': 7,
            'G': 0,
             
        }
 
        return H_dist[n]
 
#Describe your graph here  
Graph_nodes = {
    'A': [('B', 2), ('E', 3)],
    'B': [('C', 1),('G', 9)],
    'C': None,
    'E': [('D', 6)],
    'D': [('G', 1)],
     
}
aStarAlgo('A', 'G')

        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 9</h3>
        <pre>
            def print_board(board):
    for row in board:
        print(" | ".join(row))
        print("-" * 5)

def check_winner(board, player):
    # Check rows, columns, and diagonals
    for i in range(3):
        if all(board[i][j] == player for j in range(3)) or all(board[j][i] == player for j in range(3)):
            return True
    if all(board[i][i] == player for i in range(3)) or all(board[i][2 - i] == player for i in range(3)):
        return True
    return False

def is_board_full(board):
    return all(board[i][j] != " " for i in range(3) for j in range(3))

def tic_tac_toe():
    board = [[" " for _ in range(3)] for _ in range(3)]
    players = ["X", "O"]
    current_player = players[0]

    while True:
        print_board(board)

        # Get player move
        while True:
            row = int(input("Enter row (0, 1, or 2): "))
            col = int(input("Enter column (0, 1, or 2): "))
            if 0 <= row < 3 and 0 <= col < 3 and board[row][col] == " ":
                break
            else:
                print("Invalid move. Try again.")

        # Make the move
        board[row][col] = current_player

        # Check for a winner
        if check_winner(board, current_player):
            print_board(board)
            print(f"Player {current_player} wins!")
            break

        # Check for a tie
        if is_board_full(board):
            print_board(board)
            print("It's a tie!")
            break

        # Switch to the other player
        current_player = players[1] if current_player == players[0] else players[0]

if __name__ == "__main__":
    tic_tac_toe()


        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 10</h3>
        <pre>
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_test = StandardScaler().fit_transform(X_train), StandardScaler().fit_transform(X_test)
model_single_layer = models.Sequential([layers.Dense(64, 'relu', input_shape=(4,)), layers.Dense(3, 'softmax')])
model_single_layer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model_single_layer.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test))
single_layer_accuracy = accuracy_score(y_test, np.argmax(model_single_layer.predict(X_test), axis=1))
print(f"\nSingle-layer Neural Network - Accuracy: {single_layer_accuracy}")
model_multi_layer = models.Sequential([layers.Dense(64, 'relu', input_shape=(4,)), layers.Dense(32, 'relu'), layers.Dense(3, 'softmax')])
model_multi_layer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model_multi_layer.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test))
multi_layer_accuracy = accuracy_score(y_test, np.argmax(model_multi_layer.predict(X_test), axis=1))
print(f"\nMulti-layer Neural Network - Accuracy: {multi_layer_accuracy}")
        </pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 11</h3>
        <pre>

        
        import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)
X_train = np.random.rand(100, 1)
y_train = 2 * X_train + 1 + 0.1 * np.random.randn(100, 1)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=1, input_shape=(1,))
])
model.compile(optimizer='sgd', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, verbose=0)
plt.plot(history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error Loss')
plt.title('Training Loss')
plt.show()
X_test = np.array([[0.2], [0.5], [0.8]])
predictions = model.predict(X_test)
for i in range(len(X_test)):
    print(f"Input: {X_test[i][0]}, Predicted Output: {predictions[i][0]}")
</pre>
    </div><br/><hr/>
    <div class="codes">
        <h3>Exp 12</h3>
        <pre>
            import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D
# Load and preprocess the MNIST dataset
mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize pixel values
# Define the CNN model
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
# Train the model
model.fit(X_train, y_train, epochs=5, validation_split=0.1)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'\nTest accuracy: {test_acc}')


        </pre>
    </div><br/><hr/>
</div>

<div class="datasets">
    <h3 style="text-decoration:overline;">Datasets üìÇ</h3>
    <div class="ds1">
        <button id="btn_down1" onclick="downloadFile('abcde.csv')">
            abcde üì©
        </button>
    </div>
    <div class="ds2">
        <button id="btn_down2" onclick="downloadFile('diabetes.csv')">
            diabetes üì©
        </button>
    </div>
    <div class="ds3">
        <button id="btn_down3" onclick="downloadFile('iris.csv')">
            iris üì©
        </button>
    </div>
    <div class="ds4">
        <button id="btn_down4" onclick="downloadFile('Social_Network_Ads.csv')">
            Social Network Ads üì©
        </button>
    </div>
    <div class="ds5"></div>
    <button id="btn_down5" onclick="downloadFile('Dataset_exp1.csv')">
       Dataset Exp1 üì©
    </button>
    <div class="ds6">
        <button id="btn_down4" onclick="downloadFile('decision_tree.csv')">
            Decision Tree üì©
        </button>
    </div>
</div>

<script>
    function downloadFile(filename) {
        var anchor = document.createElement('a');
        anchor.setAttribute('href', filename);
        anchor.setAttribute('download', filename);
        anchor.click();
    }
</script>

</body>
</html>
